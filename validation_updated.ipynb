{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import os\n",
    "import pandas as pd\n",
    "import re, datetime\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starbucks 2016-05-05.xlsx\n",
      "Starbucks 2016-09-12(Bakery).xlsx\n",
      "Starbucks 2016-11-05(bakery).xlsx\n",
      "Starbucks 2016-12-17.xlsx\n",
      "Starbucks 2017-01-02.xlsx\n",
      "Starbucks 2017-03-04.xlsx\n",
      "Starbucks 2017-05-05.xlsx\n",
      "Starbucks 2017-06-04.xlsx\n",
      "Starbucks 2017-10-22.xlsx\n",
      "Starbucks 2018-02-16.xlsx\n",
      "Starbucks 2018-04-20(Fixed 05-06-18).xlsx\n",
      "Starbucks 2018-05-20.xlsx\n",
      "Starbucks 2018-06-18.xlsx\n",
      "Starbucks 2018-10-27.xlsx\n",
      "Starbucks 2019-02-18.xlsx\n",
      "Starbucks 2019-04-17.xlsx\n",
      "Starbucks 2019-05-18.xlsx\n",
      "Starbucks 2019-07-02.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['2016-05-05'], 'Starbucks 2016-05-05.xlsx'],\n",
       " [['2016-09-12'], 'Starbucks 2016-09-12(Bakery).xlsx'],\n",
       " [['2016-11-05'], 'Starbucks 2016-11-05(bakery).xlsx'],\n",
       " [['2016-12-17'], 'Starbucks 2016-12-17.xlsx'],\n",
       " [['2017-01-02'], 'Starbucks 2017-01-02.xlsx'],\n",
       " [['2017-03-04'], 'Starbucks 2017-03-04.xlsx'],\n",
       " [['2017-05-05'], 'Starbucks 2017-05-05.xlsx'],\n",
       " [['2017-06-04'], 'Starbucks 2017-06-04.xlsx'],\n",
       " [['2017-10-22'], 'Starbucks 2017-10-22.xlsx'],\n",
       " [['2018-02-16'], 'Starbucks 2018-02-16.xlsx'],\n",
       " [['2018-04-20'], 'Starbucks 2018-04-20(Fixed 05-06-18).xlsx'],\n",
       " [['2018-05-20'], 'Starbucks 2018-05-20.xlsx'],\n",
       " [['2018-06-18'], 'Starbucks 2018-06-18.xlsx'],\n",
       " [['2018-10-27'], 'Starbucks 2018-10-27.xlsx'],\n",
       " [['2019-02-18'], 'Starbucks 2019-02-18.xlsx'],\n",
       " [['2019-04-17'], 'Starbucks 2019-04-17.xlsx'],\n",
       " [['2019-05-18'], 'Starbucks 2019-05-18.xlsx'],\n",
       " [['2019-07-02'], 'Starbucks 2019-07-02.xlsx']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out input files, which will be sotred by date\n",
    "files = []\n",
    "path = \"E:\\shar\"\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    print(file)\n",
    "    files.append(list(\n",
    "    ([str(i)[0:10] for i in file.split() if re.search('\\d{4}-\\d{2}-\\d{2}', i)], file)\n",
    "        ))\n",
    "#sort by datetime by asending order\n",
    "files.sort(key=lambda L: datetime.datetime.strptime(L[0][0], '%Y-%m-%d'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert needed columns into list\n",
    "def makeList(file):\n",
    "    item_list = []\n",
    "    file[['Item Type 1','Item Type 2']] = file[['Item Type 1','Item Type 2']].replace(np.nan, '', regex=True)\n",
    "    file['Item_Types'] = file[\"Item Type 1\"] +\" \"+ file[\"Item Type 2\"]\n",
    "    file[\"name_and_type\"] = file[\"Item_Name\"] +\" \"+ file[\"Item Type 1\"] +\" \"+ file[\"Item Type 2\"]\n",
    "\n",
    "    file[\"name_and_type\"] = file[\"name_and_type\"].str.lower()    \n",
    "    file[\"name_and_type\"] = file[\"name_and_type\"].str.rstrip()\n",
    "    #file['Serving_Unit'] = file['Serving_Unit'].astype(str)\n",
    "    \n",
    "    for j in range(len(file)):\n",
    "        item_cursor = []\n",
    "        item_cursor.append(file['name_and_type'][j])\n",
    "\n",
    "        item_cursor.append(file['Serving_Size'][j])\n",
    "        item_cursor.append(file['Serving_Unit'][j])\n",
    "        \n",
    "        item_cursor.append(file['Item_Name'][j])\n",
    "        item_cursor.append(file['Item_Types'][j])\n",
    " \n",
    "        \n",
    "        item_list.append(item_cursor)\n",
    "    return item_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starbucks 2016-05-05.xlsx\n",
      "Starbucks 2016-09-12(Bakery).xlsx\n",
      "Starbucks 2016-11-05(bakery).xlsx\n",
      "Starbucks 2016-12-17.xlsx\n",
      "Starbucks 2017-01-02.xlsx\n",
      "Starbucks 2017-03-04.xlsx\n",
      "Starbucks 2017-05-05.xlsx\n",
      "Starbucks 2017-06-04.xlsx\n",
      "Starbucks 2017-10-22.xlsx\n",
      "Starbucks 2018-02-16.xlsx\n",
      "Starbucks 2018-04-20(Fixed 05-06-18).xlsx\n",
      "Starbucks 2018-05-20.xlsx\n",
      "Starbucks 2018-06-18.xlsx\n",
      "Starbucks 2018-10-27.xlsx\n",
      "Starbucks 2019-02-18.xlsx\n",
      "Starbucks 2019-04-17.xlsx\n",
      "Starbucks 2019-05-18.xlsx\n",
      "Starbucks 2019-07-02.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['2016-05-05'], 'Starbucks 2016-05-05.xlsx'],\n",
       " [['2016-09-12'], 'Starbucks 2016-09-12(Bakery).xlsx'],\n",
       " [['2016-11-05'], 'Starbucks 2016-11-05(bakery).xlsx'],\n",
       " [['2016-12-17'], 'Starbucks 2016-12-17.xlsx'],\n",
       " [['2017-01-02'], 'Starbucks 2017-01-02.xlsx'],\n",
       " [['2017-03-04'], 'Starbucks 2017-03-04.xlsx'],\n",
       " [['2017-05-05'], 'Starbucks 2017-05-05.xlsx'],\n",
       " [['2017-06-04'], 'Starbucks 2017-06-04.xlsx'],\n",
       " [['2017-10-22'], 'Starbucks 2017-10-22.xlsx'],\n",
       " [['2018-02-16'], 'Starbucks 2018-02-16.xlsx'],\n",
       " [['2018-04-20'], 'Starbucks 2018-04-20(Fixed 05-06-18).xlsx'],\n",
       " [['2018-05-20'], 'Starbucks 2018-05-20.xlsx'],\n",
       " [['2018-06-18'], 'Starbucks 2018-06-18.xlsx'],\n",
       " [['2018-10-27'], 'Starbucks 2018-10-27.xlsx'],\n",
       " [['2019-02-18'], 'Starbucks 2019-02-18.xlsx'],\n",
       " [['2019-04-17'], 'Starbucks 2019-04-17.xlsx'],\n",
       " [['2019-05-18'], 'Starbucks 2019-05-18.xlsx'],\n",
       " [['2019-07-02'], 'Starbucks 2019-07-02.xlsx']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_files = []\n",
    "path = \"E:/shar/\"\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    print(file)\n",
    "    pre_files.append(list(\n",
    "    ([str(i)[0:10] for i in file.split() if re.search('\\d{4}-\\d{2}-\\d{2}', i)], file)\n",
    "        ))\n",
    "#sort by datetime by asending order\n",
    "pre_files.sort(key=lambda L: datetime.datetime.strptime(L[0][0], '%Y-%m-%d'))\n",
    "pre_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"E:/shar/\"\n",
    "file_name = pre_files[0][1]\n",
    "pre_file = pd.read_excel(path+str(pre_files.pop(0)[1]))\n",
    "# It's bad to include space in column name in python, so we'd better rename it\n",
    "pre_file  = pre_file.rename(columns={pre_file.columns[4]: \"Item_Name\",\n",
    "                                    pre_file.columns[12]: \"Serving_Size\",\n",
    "                                    pre_file.columns[13]: \"Serving_Unit\"})\n",
    "# In case sometimes the columns won't stay in designated order, find the columns by name and then rename it like below\n",
    "#pre_file = pre_file.rename(columns={\"Item Name\": \"Item_Name\", \"Serving Size\": \"Serving_Size\", \"Serving Unit\": \"Serving_Unit\"}, errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns for fuzzy matching (80% similar)\n",
    "pre_file[\"pre_fuzzy\"] = np.nan\n",
    "pre_file['pre_fuzzy_item_name'] = np.nan\n",
    "pre_file['pre_fuzzy_item_types'] = np.nan\n",
    "pre_file['pre_fuzzy_loc'] = np.nan\n",
    "pre_file['pre_fuzzy_ser_size'] = np.nan\n",
    "pre_file['pre_fuzzy_ser_unit'] = np.nan\n",
    "pre_file['pre_fuzzy_size_match'] = np.nan\n",
    "pre_file['pre_fuzzy_unit_match'] = np.nan\n",
    "\n",
    "\n",
    "# columns for exact matching (100% identical)\n",
    "pre_file['pre_exact'] = np.nan\n",
    "pre_file['pre_exact_item_name'] = np.nan\n",
    "pre_file['pre_exact_types'] = np.nan\n",
    "pre_file['pre_exact_loc'] = np.nan\n",
    "pre_file['pre_exact_ser_size'] = np.nan\n",
    "pre_file['pre_exact_ser_unit'] = np.nan\n",
    "pre_file['pre_exact_size_match'] = np.nan\n",
    "pre_file['pre_exact_unit_match'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = files[0][1]\n",
    "pre_file.to_excel(\"E:/mid-outcome/\"+ file_name)\n",
    "list_pre = makeList(pre_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare $t$ with $t-1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 12 is out of bounds for axis 0 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-f7db11ca26cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mcurrent_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     current_file = current_file.rename(columns={current_file.columns[4]: \"Item_Name\",\n\u001b[1;32m----> 9\u001b[1;33m                                     \u001b[0mcurrent_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Serving_Size\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                                     current_file.columns[13]: \"Serving_Unit\"})\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#current_file = current_file.rename(columns={\"Item Name\": \"Item_Name\", \"Serving Size\": \"Serving_Size\", \"Serving Unit\": \"Serving_Unit\"}, errors=\"raise\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4295\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4296\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4297\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4299\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 12 is out of bounds for axis 0 with size 7"
     ]
    }
   ],
   "source": [
    "# Loop begins\n",
    "%time\n",
    "path = \"E:/shar/\"\n",
    "\n",
    "while pre_files:\n",
    "    x=pre_files.pop(0)[1]\n",
    "    current_file = pd.read_excel(path+x)\n",
    "    current_file = current_file.rename(columns={current_file.columns[4]: \"Item_Name\",\n",
    "                                    current_file.columns[12]: \"Serving_Size\",\n",
    "                                    current_file.columns[13]: \"Serving_Unit\"})\n",
    "    #current_file = current_file.rename(columns={\"Item Name\": \"Item_Name\", \"Serving Size\": \"Serving_Size\", \"Serving Unit\": \"Serving_Unit\"}, errors=\"raise\")\n",
    "    \n",
    "    \n",
    "    current_list = makeList(current_file)\n",
    "    \n",
    "    current_file[\"pre_fuzzy\"] = 0\n",
    "    current_file['pre_fuzzy_item_name'] = np.nan\n",
    "    current_file['pre_fuzzy_types'] = np.nan\n",
    "    current_file['pre_fuzzy_loc'] = np.nan\n",
    "    current_file['pre_fuzzy_ser_size'] = np.nan\n",
    "    current_file['pre_fuzzy_ser_unit'] = np.nan\n",
    "    current_file['pre_fuzzy_size_match'] = np.nan\n",
    "    current_file['pre_fuzzy_unit_match'] = np.nan\n",
    "    \n",
    "    current_file['pre_exact'] = 0\n",
    "    current_file['pre_exact_item_name'] = np.nan\n",
    "    current_file['pre_exact_types'] = np.nan\n",
    "    current_file['pre_exact_loc'] = np.nan\n",
    "    current_file['pre_exact_ser_size'] = np.nan\n",
    "    current_file['pre_exact_ser_unit'] = np.nan\n",
    "    current_file['pre_exact_size_match'] = np.nan\n",
    "    current_file['pre_exact_unit_match'] = np.nan\n",
    "    \n",
    "                   \n",
    "    matched_fuzzy = []\n",
    "    matched_exact = []\n",
    "    \n",
    "    for i in current_list:\n",
    "        # Default similiarity criteria\n",
    "        max_sim = 85\n",
    "        max_j = []\n",
    "        for j in list_pre:\n",
    "            Token_Sort_Ratio = fuzz.token_sort_ratio(i[0],j[0])\n",
    "            if Token_Sort_Ratio > max_sim:\n",
    "                max_sim = Token_Sort_Ratio\n",
    "                max_j = j\n",
    "            if Token_Sort_Ratio == 100:\n",
    "                matched_exact.append([i[0],j[0],list_pre.index(j)+1,j[1],j[2],j[3], j[4]])\n",
    "        if max_j:\n",
    "            matched_fuzzy.append([i[0],max_j[0],max_sim, list_pre.index(max_j)+1, max_j[1],max_j[2],max_j[3], max_j[4]])     \n",
    "    matched_fuzzy = pd.DataFrame(matched_fuzzy)       \n",
    "    matched_fuzzy = matched_fuzzy.sort_values(matched_fuzzy.columns[2], ascending=False).drop_duplicates(matched_fuzzy.columns[1])\n",
    "    matched_fuzzy = matched_fuzzy.drop(matched_fuzzy.columns[2], axis = 1)   \n",
    "    matched_fuzzy = matched_fuzzy.values.tolist()\n",
    "            \n",
    "        \n",
    "    Row_list = []\n",
    "    for row in current_file.itertuples():\n",
    "            my_list =[row.name_and_type,row.Serving_Size, row.Serving_Unit, #[0], [1], [2]\n",
    "                      row.pre_fuzzy,#[3]\n",
    "                      row.pre_fuzzy_item_name,row.pre_fuzzy_types,row.pre_fuzzy_loc, # [4], [5], [6]\n",
    "                      row.pre_fuzzy_ser_size, row.pre_fuzzy_ser_unit, # [7],[8]\n",
    "                      row.pre_fuzzy_size_match,row.pre_fuzzy_unit_match, # [9], [10]\n",
    "                      row.pre_exact,#[11]\n",
    "                      row.pre_exact_item_name,row.pre_exact_types,row.pre_exact_loc, # [12], [13], [14]\n",
    "                      row.pre_exact_ser_size, row.pre_exact_ser_unit, #[15],[16]\n",
    "                      row.pre_exact_size_match,row.pre_exact_unit_match] # [17], [18]\n",
    "            Row_list.append(my_list)        \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    #Convert row_list into lower case and ingore space\n",
    "    for i in range(len(Row_list)): \n",
    "        if type(Row_list[i][0]) == str:\n",
    "            Row_list[i][0] = Row_list[i][0].lower()\n",
    "            Row_list[i][0] = Row_list[i][0].rstrip()\n",
    "         \n",
    "   \n",
    "    for row in Row_list:\n",
    "        for k in matched_fuzzy:\n",
    "            if row[0] == k[0]:\n",
    "                # binary indentifier\n",
    "                row[3]= 1\n",
    "                # item name\n",
    "                row[4]= k[5]\n",
    "                # item type \n",
    "                row[5] = k[6]\n",
    "                # item loc\n",
    "                row[6] = k[2]\n",
    "                # serving size\n",
    "                row[7] = k[3]\n",
    "                # serving unit\n",
    "                row[8] = k[4]\n",
    "                \n",
    "                # Compare serving size (consier NA here)\n",
    "                # both are not na\n",
    "                if pd.isnull(row[1]) != True and pd.isnull(k[3]) != True:\n",
    "                    if row[1] == k[3]:\n",
    "                        row[9] = 1\n",
    "                    else:\n",
    "                        row[9] = 0\n",
    "                \n",
    "                # if one is na while other is not\n",
    "                elif pd.isnull(row[1]) and pd.isnull(k[3]) != True:\n",
    "                    row[9] = 0   \n",
    "                elif pd.isnull(row[1]) != True and pd.isnull(k[3]):\n",
    "                    row[9] = 0\n",
    "                else:\n",
    "                    row[9] = np.nan\n",
    "                    \n",
    "                # Compare serving unit\n",
    "                if pd.isnull(row[2]) != True and pd.isnull(k[4]) != True:\n",
    "                    if row[2] == k[4]:\n",
    "                        row[10] = 1\n",
    "                    else:\n",
    "                        row[10] = 0\n",
    "                elif pd.isnull(row[2]) and pd.isnull(k[4]) != True:\n",
    "                    row[10] = 0\n",
    "                elif pd.isnull(row[2])!= True and pd.isnull(k[4]):\n",
    "                    row[10] = 0\n",
    "                else:\n",
    "                    row[10] = np.nan\n",
    "                        \n",
    " \n",
    "        # Same thing to exact matching here       \n",
    "        for h in matched_exact:\n",
    "            if row[0] == h[0]:\n",
    "                # binary indentifier\n",
    "                row[11] = 1\n",
    "                # item name\n",
    "                row[12] = h[5]\n",
    "                # Item type\n",
    "                row[13] = h[6]\n",
    "                # item loc\n",
    "                row[14] = h[2]\n",
    "                # serving size\n",
    "                row[15] = h[3]\n",
    "                # serving unit\n",
    "                row[16] = h[4]\n",
    "                # Comparing Serving Size\n",
    "                if pd.isnull(row[15]) != True and pd.isnull(h[3]) != True:\n",
    "                    if row[1] == h[3]:\n",
    "                        row[17] = 1\n",
    "                    else:\n",
    "                        row[17] = 0\n",
    "                # if either one is na\n",
    "                elif pd.isnull(row[15]) and pd.isnull(h[3]) != True:\n",
    "                    row[17] = 0   \n",
    "                elif pd.isnull(row[15]) != True and pd.isnull(h[3]):\n",
    "                    row[17] = 0\n",
    "                # if both are missing\n",
    "                else:\n",
    "                    row[17] = np.nan\n",
    "                    \n",
    "                # Compare serving unit\n",
    "                if pd.isnull(row[16]) != True and pd.isnull(h[4]) != True:\n",
    "                    if row[16] == h[4]:\n",
    "                        row[18] = 1\n",
    "                    else:\n",
    "                        row[18] = 0\n",
    "                elif pd.isnull(row[16]) and pd.isnull(h[4]) != True:\n",
    "                    row[18] = 0\n",
    "                elif pd.isnull(row[16])!= True and pd.isnull(h[4]):\n",
    "                    row[18] = 0\n",
    "                else:\n",
    "                    row[18] = np.nan\n",
    "                    \n",
    "        #del row[0:3]\n",
    "\n",
    "  \n",
    "    current_file[\"pre_fuzzy\"] = 0\n",
    "    current_file['pre_fuzzy_item_name'] = np.nan\n",
    "    current_file['pre_fuzzy_types'] = np.nan\n",
    "    current_file['pre_fuzzy_loc'] = np.nan\n",
    "    current_file['pre_fuzzy_ser_size'] = np.nan\n",
    "    current_file['pre_fuzzy_ser_unit'] = np.nan\n",
    "    current_file['pre_fuzzy_size_match'] = np.nan\n",
    "    current_file['pre_fuzzy_unit_match'] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    label = pd.DataFrame(Row_list,columns=['name_and_type','Serving_Size','Serving_Unit',\n",
    "                                           'pre_fuzzy','pre_fuzzy_item_name','pre_fuzzy_types','pre_fuzzy_loc',\n",
    "                                           'pre_fuzzy_ser_size','pre_fuzzy_ser_unit',\n",
    "                                           'pre_fuzzy_size_match','pre_fuzzy_unit_match',\n",
    "                                           'pre_exact','pre_exact_item_name','pre_exact_types','pre_exact_loc',\n",
    "                                          'pre_exact_ser_size','pre_exact_ser_unit',\n",
    "                                          'pre_exact_size_match','pre_exact_unit_match'])\n",
    "    \n",
    "    label = label.drop(['name_and_type','Serving_Size','Serving_Unit'], axis = 1) \n",
    "    \n",
    "    \n",
    "    current_file[['pre_fuzzy','pre_fuzzy_item_name','pre_fuzzy_types','pre_fuzzy_loc',\n",
    "                                           'pre_fuzzy_ser_size','pre_fuzzy_ser_unit',\n",
    "                                           'pre_fuzzy_size_match','pre_fuzzy_unit_match',\n",
    "                                           'pre_exact','pre_exact_item_name','pre_exact_types','pre_exact_loc',\n",
    "                                          'pre_exact_ser_size','pre_exact_ser_unit',\n",
    "                                          'pre_exact_size_match','pre_exact_unit_match']] = label\n",
    "                \n",
    "     \n",
    "        \n",
    "    directory = 'E:/mid-outcome'\n",
    "    file = x\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    current_file.drop(['Item_Types', 'name_and_type'], axis=1).to_excel(os.path.join(directory, file))\n",
    "        \n",
    "        \n",
    "    list_pre = current_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Starbucks 2019-07-02.xlsx'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare $t$ with $t+1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_files = []\n",
    "path = \"E:/mid-outcome/\"\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    print(file)\n",
    "    post_files.append(list(\n",
    "    ([str(i)[0:10] for i in file.split() if re.search('\\d{4}-\\d{2}-\\d{2}', i)], file)\n",
    "        ))\n",
    "#sort by datetime by asending order\n",
    "post_files.sort(key=lambda L: datetime.datetime.strptime(L[0][0], '%Y-%m-%d'))\n",
    "post_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = post_files[-1][1]\n",
    "next_file = pd.read_excel(path+post_files.pop()[1])\n",
    "\n",
    " # columns for fuzzy matching (80% similar)\n",
    "next_file[\"post_fuzzy\"] = 0\n",
    "next_file['post_fuzzy_item_name'] = np.nan\n",
    "next_file['post_fuzzy_types'] = np.nan\n",
    "next_file['post_fuzzy_loc'] = np.nan\n",
    "next_file['post_fuzzy_ser_size'] = np.nan\n",
    "next_file['post_fuzzy_ser_unit'] = np.nan\n",
    "next_file['post_fuzzy_size_match'] = np.nan\n",
    "next_file['post_fuzzy_unit_match'] = np.nan\n",
    "\n",
    "# columns for exact matching (100% identical)\n",
    "next_file['post_exact'] = 0\n",
    "next_file['post_exact_item_name'] = np.nan\n",
    "next_file['post_exact_types'] = np.nan\n",
    "next_file['post_exact_loc'] = np.nan\n",
    "next_file['post_exact_ser_size'] = np.nan\n",
    "next_file['post_exact_ser_unit'] = np.nan\n",
    "next_file['post_exact_size_match'] = np.nan\n",
    "next_file['post_exact_unit_match'] = np.nan\n",
    "\n",
    "next_file.to_excel(\"E:/mid-outcome/\"+ file_name)\n",
    "list_next = makeList(next_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "while post_files:\n",
    "    x=post_files.pop()[1]\n",
    "    current_file = pd.read_excel(path+x)\n",
    "    current_list = makeList(current_file)\n",
    "\n",
    "    \n",
    "    # columns for fuzzy matching (85% similar)\n",
    "    current_file[\"post_fuzzy\"] = 0\n",
    "    current_file['post_fuzzy_item_name'] = np.nan\n",
    "    current_file['post_fuzzy_types'] = np.nan\n",
    "    current_file['post_fuzzy_loc'] = np.nan\n",
    "    current_file['post_fuzzy_ser_size'] = np.nan\n",
    "    current_file['post_fuzzy_ser_unit'] = np.nan\n",
    "    current_file['post_fuzzy_size_match'] = np.nan\n",
    "    current_file['post_fuzzy_unit_match'] = np.nan\n",
    "\n",
    "\n",
    "    # columns for exact matching (100% identical)\n",
    "    current_file['post_exact'] = 0\n",
    "    current_file['post_exact_item_name'] = np.nan\n",
    "    current_file['post_exact_types'] = np.nan\n",
    "    current_file['post_exact_loc'] = np.nan\n",
    "    current_file['post_exact_ser_size'] = np.nan\n",
    "    current_file['post_exact_ser_unit'] = np.nan\n",
    "    current_file['post_exact_size_match'] = np.nan\n",
    "    current_file['post_exact_unit_match'] = np.nan\n",
    "\n",
    "\n",
    "                \n",
    "    matched_fuzzy = []\n",
    "    matched_exact = []\n",
    "    \n",
    "    for i in current_list:\n",
    "        max_sim = 85\n",
    "        max_j = []\n",
    "        for j in list_next:\n",
    "            #comparing item name and item types\n",
    "            Token_Sort_Ratio = fuzz.token_sort_ratio(i[0],j[0])\n",
    "            if Token_Sort_Ratio > max_sim:\n",
    "                max_sim = Token_Sort_Ratio\n",
    "                max_j = j\n",
    "            if Token_Sort_Ratio == 100:\n",
    "                matched_exact.append([i[0],j[0],list_next.index(j)+1,j[1],j[2],j[3], j[4]])\n",
    "        if max_j:\n",
    "            matched_fuzzy.append([i[0],max_j[0],max_sim, list_next.index(max_j)+1, max_j[1],max_j[2],max_j[3], max_j[4]]) \n",
    "    matched_fuzzy = pd.DataFrame(matched_fuzzy)       \n",
    "    matched_fuzzy = matched_fuzzy.sort_values(matched_fuzzy.columns[2], ascending=False).drop_duplicates(matched_fuzzy.columns[1])\n",
    "    matched_fuzzy = matched_fuzzy.drop(matched_fuzzy.columns[2], axis = 1)   \n",
    "    matched_fuzzy = matched_fuzzy.values.tolist()\n",
    "    # j[0] is the matched item, list_pre.index(j[0])+1] is its location\n",
    "    # j[1] is the serving size,  j[2] is serving unit\n",
    "    # j[3] is the item name   # j[4] is the item types\n",
    "                              \n",
    "            \n",
    "        \n",
    "    Row_list = []\n",
    "    for row in current_file.itertuples():\n",
    "            my_list =[row.name_and_type,row.Serving_Size, row.Serving_Unit, #[0], [1], [2]\n",
    "                      row.post_fuzzy,#[3]\n",
    "                      row.post_fuzzy_item_name, row.post_fuzzy_types,row.post_fuzzy_loc, # [4], [5], [6]\n",
    "                      row.post_fuzzy_ser_size, row.post_fuzzy_ser_unit, # [7],[8]\n",
    "                      row.post_fuzzy_size_match,row.post_fuzzy_unit_match, # [9], [10]\n",
    "                      row.post_exact, #[11]\n",
    "                      row.post_exact_item_name,row.post_exact_types,row.post_exact_loc,  # [12], [13], [14]\n",
    "                      row.post_exact_ser_size, row.post_exact_ser_unit, # [15], [16]\n",
    "                      row.post_exact_size_match,row.post_exact_unit_match] # [17], [18]\n",
    "            Row_list.append(my_list)        \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    #Convert row_list into lower case and ingore space\n",
    "    for i in range(len(Row_list)): \n",
    "        if type(Row_list[i][0]) == str:\n",
    "            Row_list[i][0] = Row_list[i][0].lower()\n",
    "            Row_list[i][0] = Row_list[i][0].rstrip()\n",
    "         \n",
    "   \n",
    "    for row in Row_list:\n",
    "        for k in matched_fuzzy:\n",
    "            if row[0] == k[0]:\n",
    "                # binary indentifier\n",
    "                row[3]= 1\n",
    "                # item name\n",
    "                row[4]= k[5]\n",
    "                # item type\n",
    "                row[5] = k[6]\n",
    "                # item loc\n",
    "                row[6] = k[2]\n",
    "                # serving size\n",
    "                row[7] = k[3]\n",
    "                # serving unit\n",
    "                row[8] = k[4]\n",
    "                \n",
    "                # Compare serving size (consier NA here)\n",
    "                # both are not na\n",
    "                if pd.isnull(row[1]) != True and pd.isnull(k[3]) != True:\n",
    "                    if row[1] == k[3]:\n",
    "                        row[9] = 1\n",
    "                    else:\n",
    "                        row[9] = 0\n",
    "                \n",
    "                # if one is na while other is not\n",
    "                elif pd.isnull(row[1]) and pd.isnull(k[3]) != True:\n",
    "                    row[9] = 0   \n",
    "                elif pd.isnull(row[1]) != True and pd.isnull(k[3]):\n",
    "                    row[9] = 0\n",
    "                else:\n",
    "                    row[9] = np.nan\n",
    "                    \n",
    "                # Compare serving unit\n",
    "                if pd.isnull(row[2]) != True and pd.isnull(k[4]) != True:\n",
    "                    if row[2] == k[4]:\n",
    "                        row[10] = 1\n",
    "                    else:\n",
    "                        row[10] = 0\n",
    "                elif pd.isnull(row[2]) and pd.isnull(k[4]) != True:\n",
    "                    row[10] = 0\n",
    "                elif pd.isnull(row[2])!= True and pd.isnull(k[4]):\n",
    "                    row[10] = 0\n",
    "                else:\n",
    "                    row[10] = np.nan\n",
    "                        \n",
    " \n",
    "        # Same thing to exact matching here       \n",
    "        for h in matched_exact:\n",
    "            if row[0] == h[0]:\n",
    "                # binary indentifier\n",
    "                row[11] = 1\n",
    "                # # item name\n",
    "                row[12] = h[5]\n",
    "                # item types \n",
    "                row[13] = h[6]\n",
    "                # item loc\n",
    "                row[14] = h[2]\n",
    "                # serving size\n",
    "                row[15] = h[3]\n",
    "                # serving unit\n",
    "                row[16] = h[4]\n",
    "                # Comparing Serving Size\n",
    "                if pd.isnull(row[15]) != True and pd.isnull(h[3]) != True:\n",
    "                    if row[1] == h[3]:\n",
    "                        row[17] = 1\n",
    "                    else:\n",
    "                        row[17] = 0\n",
    "                # if either one is na\n",
    "                elif pd.isnull(row[15]) and pd.isnull(h[3]) != True:\n",
    "                    row[17] = 0   \n",
    "                elif pd.isnull(row[15]) != True and pd.isnull(h[3]):\n",
    "                    row[17] = 0\n",
    "                # if both are missing\n",
    "                else:\n",
    "                    row[17] = np.nan\n",
    "                    \n",
    "                # Compare serving unit\n",
    "                if pd.isnull(row[16]) != True and pd.isnull(h[4]) != True:\n",
    "                    if row[16] == h[4]:\n",
    "                        row[18] = 1\n",
    "                    else:\n",
    "                        row[18] = 0\n",
    "                elif pd.isnull(row[16]) and pd.isnull(h[4]) != True:\n",
    "                    row[18] = 0\n",
    "                elif pd.isnull(row[16])!= True and pd.isnull(h[4]):\n",
    "                    row[18] = 0\n",
    "                else:\n",
    "                    row[16] = np.nan\n",
    "                    \n",
    "        #del row[0:3]\n",
    "\n",
    "    \n",
    "    \n",
    "    label = pd.DataFrame(Row_list,columns=['name_and_type','Serving_Size','Serving_Unit',\n",
    "                                           'post_fuzzy','post_fuzzy_item_name','post_fuzzy_types','post_fuzzy_loc',\n",
    "                                           'post_fuzzy_ser_size','post_fuzzy_ser_unit',\n",
    "                                           'post_fuzzy_size_match','post_fuzzy_unit_match',\n",
    "                                           'post_exact','post_exact_item_name','post_exact_types','post_exact_loc',\n",
    "                                          'post_exact_ser_size','post_exact_ser_unit',\n",
    "                                          'post_exact_size_match','post_exact_unit_match'])\n",
    "    \n",
    "    label = label.drop(['name_and_type','Serving_Size','Serving_Unit'], axis = 1) \n",
    "    \n",
    "    \n",
    "    current_file[['post_fuzzy','post_fuzzy_item_name','post_fuzzy_types','post_fuzzy_loc',\n",
    "                                           'post_fuzzy_ser_size','post_fuzzy_ser_unit',\n",
    "                                           'post_fuzzy_size_match','post_fuzzy_unit_match',\n",
    "                                           'post_exact','post_exact_item_name','post_exact_types','post_exact_loc',\n",
    "                                          'post_exact_ser_size','post_exact_ser_unit',\n",
    "                                          'post_exact_size_match','post_exact_unit_match']] = label\n",
    "                \n",
    "     \n",
    "        \n",
    "    directory = 'E:/mid-outcome'\n",
    "    file = x\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    current_file.drop(['Item_Types', 'name_and_type'], axis=1).to_excel(os.path.join(directory, file))\n",
    "        \n",
    "        \n",
    "    list_next = current_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
